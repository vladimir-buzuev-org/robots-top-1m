#Robots.txt File 

# Crawlers Setup
User-agent: *

# Website Sitemap
Sitemap: http://joby.com/sitemap/joby/sitemap.xml
Sitemap: http://store.lowepro.com/sitemap/lowepro/sitemap.xml

# Directories
User-agent: *
Disallow: /app/
Disallow: /cgi-bin/
Disallow: /downloader/
Disallow: /errors/
Disallow: /includes/
Disallow: /lib/
Disallow: /shell/
Disallow: /var/
Disallow: /jslib/

# Paths (clean URLs)
User-agent: *
Disallow: /index.php/
# Disallow: /catalog/product_compare/
# Disallow: /catalog/category/view/
# Disallow: /catalog/product/view/
Disallow: /catalogsearch/
Disallow: /checkout/
Disallow: /control/
Disallow: /customer/
Disallow: /customize/
Disallow: /pkginfo/
Disallow: /review/
Disallow: /sendfriend/
Disallow: /wishlist/

# Files
User-agent: *
Disallow: /api.php
Disallow: /cron.php
Disallow: /cron.sh
Disallow: /get.php
Disallow: /LICENSE.html
Disallow: /LICENSE.txt
Disallow: /LICENSE_AFL.txt
Disallow: /mage

# Paths (no clean URLs)
User-agent: *
Disallow: /*.php$
Disallow: /*?p=*&
Disallow: /*?SID=

#Disallow bots from crawling parameters
User-agent: bingbot
User-agent: bing
User-agent: rogerbot
Disallow: /*?*