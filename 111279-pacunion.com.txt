# See http://www.robotstxt.org/wc/norobots.html for documentation on how to use the robots.txt file
#
# To ban all spiders from the entire site uncomment the next two lines:
# User-Agent: *
# Disallow: /

User-Agent: 008
Disallow: /

User-Agent: sistrix
Disallow: /

User-Agent: MJ12bot
Disallow: /

User-Agent: Baiduspider
Disallow: /

User-Agent: Yandex
Disallow: /

User-Agent: Ezooms
Disallow: /

User-Agent: Linguee
Disallow: /

User-Agent: SeznamBot
disallow: /

User-Agent: businessdbbot
disallow: /

User-Agent: AhrefsBot
disallow: /

User-Agent: Spinn3r
disallow: /

User-Agent: psbot
Disallow: /

User-Agent: R6_CommentReader
Disallow: /

User-Agent: TurnitinBot
Disallow: /

User-Agent: JikeSpider
Disallow: /

User-Agent: oBot
Disallow: /

User-Agent: PaperLiBot
Disallow: /

User-Agent: Exabot
Disallow: /

User-Agent: WBSearchBot
Disallow: /

User-Agent: SEOkicks-Robot
Disallow: /

User-Agent: discoverybot
Disallow: /

User-Agent: Genieo
Disallow: /

User-Agent: meanpathbot
Disallow: /

User-Agent: *
Disallow: /admin/autocompletions/agents_for_display

User-Agent: *
Disallow: /onboard_schools

User-Agent: *
Disallow: /showings/new

User-Agent: *
Disallow: /inquiries/new

User-Agent: *
Disallow: /email_friends/new

User-agent: ScoutJet
Crawl-delay: 5

User-agent: AppleNewsBot
Crawl-delay: 5

Sitemap: http://www.pacificunion.com/site_map.xml
