# Secure Computing: robots.txt
#
# This file is used to allow crawlers to index our site, eliminate links to duplicate content, and keep them from getting stuck in infinite loops.
#
User-agent: *
Disallow: /cgi-bin/
Disallow: /download/
Disallow: /goto/
Disallow: /pdf/
Disallow: /*&pf=1
Disallow: /*&lang=en
Disallow: /*&menu=
