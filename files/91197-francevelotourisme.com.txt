# Define access-restrictions for robots/spiders
# http://www.robotstxt.org/wc/norobots.html



# By default we allow robots to access all areas of our site
# already accessible to anonymous users

User-agent: *
Disallow:
Request-rate: 1/10
Crawl-delay: 10
Visit-time: 0100-0500



# Add Googlebot-specific syntax extension to exclude forms
# that are repeated for each piece of content in the site
# the wildcard is only supported by Googlebot
# http://www.google.com/support/webmasters/bin/answer.py?answer=40367&ctx=sibling

User-agent: Googlebot
Disallow: /*sendto_form$
Disallow: /*folder_factories$

User-agent: Mediapartners-Google
Disallow: /*sendto_form$
Disallow: /*folder_factories$

User-agent: Yahoo! Slurp
Disallow: /*sendto_form$
Disallow: /*folder_factories$
Request-rate: 1/10
Crawl-delay: 10
Visit-time: 0100-0600

User-agent: bingbot
Disallow: /*sendto_form$
Disallow: /*folder_factories$
Request-rate: 1/10
Crawl-delay: 10
Visit-time: 0100-0600

User-agent: Baiduspider
Disallow: /*sendto_form$
Disallow: /*folder_factories$
Request-rate: 1/10
Crawl-delay: 10
Visit-time: 0100-0600

User-agent: msnbot
Disallow: /

# Block MJ12bot as it is just noise
User-agent: MJ12bot
Disallow: /

# Block Ahrefs
User-agent: AhrefsBot
Disallow: /

# Block Sogou
User-agent: sogou spider
Disallow: /

# Block SEOkicks
User-agent: SEOkicks-Robot
Disallow: /

# SEOkicks
User-agent: SEOkicks
Disallow: /

# Dicoveryengine.com
User-agent: discobot
Disallow: /

# Blekkobot
User-agent: Blekkobot
Disallow: /

# Block BlexBot
User-agent: BLEXBot
Disallow: /

# Block SISTRIX
User-agent: SISTRIX Crawler
Disallow: /

# Block Uptime robot
User-agent: UptimeRobot/2.0
Disallow: /

User-agent: 008
Disallow: /

# Block Ezooms Robot
User-agent: Ezooms Robot
Disallow: /

# Block Perl LWP
User-agent: Perl LWP
Disallow: /

# Block netEstate NE Crawler
User-agent: netEstate NE Crawler
Disallow: /

# Block WiseGuys Robot
User-agent: WiseGuys Robot
Disallow: /

# Block Turnitin Robot
User-agent: Turnitin Robot
Disallow: /

# Exabot
User-agent: Exabot
Disallow: /

# Yandex
User-agent: Yandex
Disallow: /

# Babya Discoverer
User-agent: Babya Discoverer
Disallow: /

# SemrushBot/1~bl
User-agent: SemrushBot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: *
# Directories
Disallow: /Plone
Disallow: /GPX
# Request-rate: defines pages/seconds to be crawled ratio. 1/20 would be 1 page in every 20 second.
# Crawl-delay: defines howmany seconds to wait after each succesful crawling.
# Visit-time: you can define between which hours you want your pages to be crawled. Example usage is: 0100-0330 which means that pages will be indexed between 01:00 AM - 03:30 AM GMT.
Request-rate: 1/10
Crawl-delay: 10
Visit-time: 0100-0500