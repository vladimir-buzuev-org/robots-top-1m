
﻿# Archivo robots.txt para NPP
# Bloqueamos primero lo global
# Fecha: 18-09-2017

#######################
# Los robots tienen acceso limitado a todo el sitio mediante el parámetro Disallow,
# excepto para las secciones, rutas o archivos especificados con el parámetro Allow

User-agent: *
Disallow: /logboxcorporativo
Disallow: /empleo-trabajo/*.html?
Disallow: /ofertas-trabajo-empleo
Disallow: /*.php
Disallow: /files
Disallow: /searchresponsive
Disallow: /premium-user
Disallow: /profile
Disallow: /pages/responsivo/enviarEmailCV.php*
Disallow: /pages/responsivo/show_cv_v2.3.php*
Disallow: /update-cv
Disallow: /pages/responsivo/set-privacy.php*

User-agent: Googlebot
Disallow: /files/*
Disallow: /premium-user
Disallow: /profile
Disallow: /pages/responsivo/enviarEmailCV.php*
Disallow: /pages/responsivo/show_cv_v2.3.php*
Disallow: /update-cv
Disallow: /pages/responsivo/set-privacy.php*
Disallow: /searchresponsive
Disallow: /ofertas-trabajo-empleo

#######################

#
# Bloqueamos los motores que pueden realizar una indexacion inadecuada
# Podemos agregar otros que se conozca a nivel de servidor
#

User-agent: MSIECrawler
Disallow: /

User-agent: WebCopier
Disallow: /

User-agent: HTTrack
Disallow: /

User-agent: Microsoft.URL.Control
Disallow: /

User-agent: libwww
Disallow: /

User-agent: Mediapartners-Google
Disallow: /

#
# Limite a todos los Motores en segundos
#

User-agent: *
Crawl-delay: 10
Allow: /1436