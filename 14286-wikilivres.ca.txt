# robots.txt for https://biblio.wiki

# advertising-related bots:
User-agent: Mediapartners-Google*
Disallow: /

User-agent: sitecheck.internetseer.com
Disallow: /

User-agent: Zealbot
Disallow: /

User-agent: MSIECrawler
Disallow: /

User-agent: SiteSnagger
Disallow: /

User-agent: WebStripper
Disallow: /

User-agent: WebCopier
Disallow: /

User-agent: Fetch
Disallow: /

User-agent: Offline Explorer
Disallow: /

User-agent: Teleport
Disallow: /

User-agent: TeleportPro
Disallow: /

User-agent: WebZIP
Disallow: /

User-agent: linko
Disallow: /

User-agent: HTTrack
Disallow: /

User-agent: Microsoft.URL.Control
Disallow: /

User-agent: Xenu
Disallow: /

User-agent: larbin
Disallow: /

User-agent: libwww
Disallow: /

User-agent: ZyBORG
Disallow: /

User-agent: Download Ninja
Disallow: /

# Misbehaving: requests much too fast
User-agent: fast
Disallow: /

# Sorry, wget in its recursive mode is a frequent problem.
User-agent: wget
Disallow: /

# The 'grub' distributed client has been *very* poorly behaved.
User-agent: grub-client
Disallow: /

# Doesn't follow robots.txt anyway, but...
User-agent: k2spider
Disallow: /

# Hits many times per second http://www.nameprotect.com/botinfo.html
User-agent: NPBot
Disallow: /

# A capture bot, downloads gazillions http://www.webreaper.net/
User-agent: WebReaper
Disallow: /

# Allow the Internet Archiver to index action=raw and thereby store the raw wikitext of pages
User-agent: ia_archiver
Allow: /*&action=raw

# For all other agents
User-agent: *

# Include the full sitemap url for discovery
Sitemap: https://biblio.wiki/sitemap/sitemap-index-w1k1_0n3.xml

# Disallow: /filenames*
# Disallow: /directory-and-contents/*

# Disallow and allow some Mediawiki stuff
Allow: /api.php?action=mobileview&
Allow: /load.php?
Allow: /api/rest_v1/?doc
Disallow: /index.php?
Disallow: /api/
Disallow: /trap/
Disallow: /w/
Disallow: /wiki/Special:*
Disallow: /wiki/User:*
Disallow: /wiki/Media:*
Disallow: /wiki/Talk:*
Disallow: /wiki/User_talk:*
Disallow: /wiki/Project:*
Disallow: /wiki/Project_talk:*
Disallow: /wiki/File_talk:*
Disallow: /wiki/MediaWiki:*
Disallow: /wiki/MediaWiki_talk:*
Disallow: /wiki/Template:*
Disallow: /wiki/Template_talk:*
Disallow: /wiki/Help:*
Disallow: /wiki/Help_talk:*
Disallow: /wiki/Category_talk:*
Disallow: /wiki/Page_talk:*
Disallow: /wiki/Index_talk:*
Disallow: /wiki/Gadget:*
Disallow: /wiki/Gadget_talk:*
Disallow: /wiki/Gadget_definition:*
Disallow: /wiki/Gadget_definition_talk:*

# Yandex complains without this
Host: biblio.wiki

# Note: any blocked pages should also have a meta tag
# Put this in the Raw Data section of HeadSpace
# <meta name=“robots” content=“no index, nofollow”>
